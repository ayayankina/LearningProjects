# -*- coding: utf-8 -*-
"""Янкина_Алëна_Алексеевна.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRVJmw1LWjbSpRlJTHxjA9zD5AvNsZ0C

## Кластеризация

Вам предстоит поработать с датасетом слушателей и исполнителей
Задача состоит в кластеризации исполнителей по жанрам на основе данных об их прослушивании

В рабочем датасете строки - пользователи котоыре слушают исполнителей и столбцы - сами исполнители

Для каждой пары (пользователь,исполнитель) в таблице стоит число - доля (процент) прослушивания этого исполнителя выбранным пользователем.

##  Подгрузка данных
"""

import pandas as pd

df = pd.read_excel('singers_users.xlsx')
df

"""# Задание 1

Транспонируем матрицу ratings, чтобы по строкам стояли исполнители.
"""

df_transposed = df.T

df_transposed

"""Выкиньте строку под названием `user`."""

df_transposed = df_transposed.drop('user', axis=0)

df_transposed

"""Заполните пропуски нулями."""

df_transposed = df_transposed.fillna(0)

df_transposed

"""Нормализуйте данные при помощи `normalize`."""

from sklearn.preprocessing import normalize

df_normalized = normalize(df_transposed)

"""Примените KMeans на преобразованной матрице (сделайте fit, а затем вычислите кластеры при помощи predict).
Определите релевантное колчичество кластером, используя метод локтя
P.S Не используя метод локтя, а любой друой метод, опишите причину выбора этого метода и выход алгоритма
"""

from sklearn.cluster import KMeans
from matplotlib import pyplot as plt

# Определим сумму квадратов расстояний для разного количества кластеров
inertia = []
for n_clusters in range(1, 11):
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(df_normalized)
    inertia.append(kmeans.inertia_)

# Построим график зависимости суммы квадратов расстояний от числа кластеров
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Количество кластеров')
plt.ylabel('Сумма квадратов расстояний')
plt.title('Метод локтя')
plt.show()

import numpy as np

# Определение оптимального количества кластеров с использованием второй производной
second_derivative = np.diff(inertia, 2)
optimal_index = np.argmax(second_derivative) + 1
optimal_n_clusters = optimal_index + 1

# Выведем оптимальное количество кластеров
print("Оптимальное количество кластеров:", optimal_n_clusters)

# Обучаем модель KMeans с оптимальным количеством кластеров
kmeans = KMeans(n_clusters=optimal_n_clusters)
kmeans.fit(df_normalized)

# Предсказываем кластеры для каждой точки данных
cluster_labels = kmeans.predict(df_normalized)

"""Выведите на экран центры кластеров (центроиды)"""

centroids = kmeans.cluster_centers_
print("Центроиды кластеров:")
print(centroids)

"""Для каждого кластера найдем топ-10 исполнителей, наиболее близких к центроидам соотвествующего кластера.

Схожесть исполнителей будем считать по косинусной мере (spatial.distance.cosine).

Ниже для вашего удобства написана функция, принимающая на вход:
* np.array points - все точки кластера
* pt - центроид кластера
* K = 10 - число
Функция возвращает K индексов объектов (строк в массиве points), ближайших к центроиду.
"""

from scipy import spatial

def pClosest(points, pt, K=10):
    ind = [i[0] for i in sorted(enumerate(points), key=lambda x: spatial.distance.cosine(x[1], pt))]

    return ind[:K]

"""Примените функцию pClosest (или придумайте свой подход) и выведите для каждого кластера названия топ-10 исполнителей, ближайших к центроиду."""

top_10_artists = []
for i in range(optimal_n_clusters):
    cluster_indices = np.where(cluster_labels == i)[0]
    top_indices = pClosest(df_normalized[cluster_indices], centroids[i])
    top_artists = df_transposed.iloc[cluster_indices[top_indices]].index.tolist()
    top_10_artists.append(top_artists)

# Выведем топ-10 исполнителей для каждого кластера
for i, artists in enumerate(top_10_artists):
    print(f"Кластер {i+1}: {artists}")

"""Расскажите о результатах, какие можно сделать выводы

# Результат:
Этот код находит и выводит топ-10 исполнителей, наиболее близких к центроидам каждого кластера.

По результатам можно сделать выводы о наиболее подходящих артистах для каждой группы по косинусной мере. Это может помочь понять, какие жанры или стили музыки характерны для каждой категории и какие исполнители наиболее популярны или кажутся предпочтительными пользователями в каждой категории.

# Задание 2

Далее вы попробуете разные методы кластеризации для поиска кластеров. Также замеряйте время работы каждого метода.

Используйте уже сгенерированные данные для теста разных кластеризаций
Так же data[1] уже имеет метки для проверки, если необходима
"""

from matplotlib import pyplot as plt
from sklearn.datasets import make_moons

data = make_moons(n_samples=100, noise=0.1, random_state=42)

X = data[0] # координаты точек
y = data[1] # метки точек

plt.scatter(X[:,0], X[:,1], c= y)

"""Попробуйте найти кластеры при помощи KMeans"""

from sklearn.cluster import KMeans
import time

start_time = time.time()

kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
kmeans_labels = kmeans.labels_

end_time = time.time()
kmeans_time = end_time - start_time

print("Время выполнения KMeans:", kmeans_time, "секунд")

from sklearn.metrics import silhouette_score

kmeans_silhouette = silhouette_score(X, kmeans_labels)
print("Силуэтовый коэффициент для KMeans:", kmeans_silhouette)

plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')
plt.title("KMeans")
plt.show()

"""

Подберите $\varepsilon$ и min_samples в DBSCAN, чтобы наилучшим образом найти кластеры.
Ищите гиперпараметры из диапазонов:
* eps in [0.05, 0.1, 0.2, 0.28, 0.3, 0.32]
* min_samples in [4, 5, 6, 7]"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import time

best_score = -1
best_eps = None
best_min_samples = None

for eps in [0.05, 0.1, 0.2, 0.28, 0.3, 0.32]:
    for min_samples in [4, 5, 6, 7]:
        start_time = time.time()
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan_labels = dbscan.fit_predict(X)
        end_time = time.time()
        dbscan_time = end_time - start_time

        if len(set(dbscan_labels)) > 1:  # убеждаемся, что есть не менее 2 кластеров
            score = silhouette_score(X, dbscan_labels)
            if score > best_score:
                best_score = score
                best_eps = eps
                best_min_samples = min_samples
                best_dbscan_labels = dbscan_labels
                best_dbscan_time = dbscan_time

print("Наилучшие параметры для DBSCAN:")
print("eps:", best_eps)
print("min_samples:", best_min_samples)
print("Время выполнения DBSCAN:", best_dbscan_time, "секунд")

from sklearn.metrics import silhouette_score

dbscan_silhouette = silhouette_score(X, best_dbscan_labels)
print("Силуэтовый коэффициент для DBSCAN:", dbscan_silhouette)

plt.scatter(X[:, 0], X[:, 1], c=best_dbscan_labels, cmap='viridis')
plt.title("DBSCAN")
plt.show()

"""

Используйте иерархическую кластеризацию для поиска кластеров.
Задайте в методе 2 кластера. Подберите гиперпараметр linkage из списка ['ward', 'complete', 'average', 'single'], дающий наилучший результат."""

from sklearn.cluster import AgglomerativeClustering
import time

best_score = -1
best_linkage = None

for linkage in ['ward', 'complete', 'average', 'single']:
    start_time = time.time()
    clustering = AgglomerativeClustering(n_clusters=2, linkage=linkage)
    hierarchical_labels = clustering.fit_predict(X)
    end_time = time.time()
    hierarchical_time = end_time - start_time

    score = silhouette_score(X, hierarchical_labels)
    if score > best_score:
        best_score = score
        best_linkage = linkage
        best_hierarchical_labels = hierarchical_labels
        best_hierarchical_time = hierarchical_time

print("Наилучший параметр для иерархической кластеризации:")
print("linkage:", best_linkage)
print("Время выполнения иерархической кластеризации:", best_hierarchical_time, "секунд")

from sklearn.metrics import silhouette_score

hierarchical_silhouette = silhouette_score(X, best_hierarchical_labels)
print("Силуэтовый коэффициент для иерархической кластеризации:", hierarchical_silhouette)

plt.scatter(X[:, 0], X[:, 1], c=best_hierarchical_labels, cmap='viridis')
plt.title("Иерархическая кластеризация")
plt.show()

"""Сделайте выводы: какой метод сработал лучше других? какой метод сработал быстрее? есть ли метод, наилучший и по качеству, и по времени одновременно?

# Выводы:
DBSCAN имеет наименьший силуэтовый коэффициент, что может указывать на менее качественное разделение кластеров. KMeans имеет набольший силуэтовый коэффициент, но он выполнялся дольше всего. Иерархическая кластеризация также показывает хорошие результаты, и работает быстрее двух остальных методов, что делает ее и KMeans наилучшими для данного случая. Однако метода, наилучшего и по качеству, и по времени одновременно, мы не получили.

### Задание на доп бал
Попробуйте реализовать KNN или любой другой метод кластеризации
"""

from sklearn.cluster import AgglomerativeClustering

# Создаем экземпляр AgglomerativeClustering
agg_cluster = AgglomerativeClustering(n_clusters=2)

# Применяем метод кластеризации к данным
agg_labels = agg_cluster.fit_predict(X)

from sklearn.metrics import silhouette_score

# Вычисляем силуэтовый коэффициент
agg_silhouette = silhouette_score(X, agg_labels)
print("Силуэтовый коэффициент для Agglomerative Clustering:", agg_silhouette)

import matplotlib.pyplot as plt

# Визуализация кластеров
plt.scatter(X[:, 0], X[:, 1], c=agg_labels, cmap='viridis', edgecolor='k')
plt.title("Agglomerative Clustering")
plt.xlabel("Признак 1")
plt.ylabel("Признак 2")
plt.show()