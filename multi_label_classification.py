# -*- coding: utf-8 -*-
"""multi-label classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Zu-PfIG--LUetINlqIH5xc3gujHMU3E

# Модульная контрольная работа №1
#Дедлайн: 20.03.2025 (четверг), 23:59
###Работа сдается в гугл-форму, которая будет закреплена в telegram конференции группы

###Работа при сдаче в гугл форму должна называться КР_Фамилия_Имя_Отчество.ipynb

###Перед сдачей просьба не стирать вывод ячеек для того, чтобы преподаватель заново не запускал ваш код

###Архив с данными прикладывать не нужно, только ipynb.

Задание.

1. Загрузить набор данных (https://www.kaggle.com/datasets/fabiochiusano/medium-articles) либо по архивом с конференции или гугл диск. Предстоит решение задачи мультилейбл классификации. В наборе данных содержится информация о статьях с Medium, целью является предсказание тематических тегов этих статей.
2. Произвести анализ данных, отобрать данные и метки, которые вы хотите/целесообразно использовать в процессе дообучения сети. После отбора в метках должно остаться не менее 25 уникальных тегов.
3. Подобрать модель для дообучения из открытых источников (huggingface подойдет, но можно и другие источники), а также генеративную модель, на которой будет решаться задача Zero-Shot мультилейбл классификация (к использованию подойдут как Encoder-Decoder архитектуры, так и Decoder-only).
4.  Выполнить Fine-Tunning выбранной вами модели, оценить ее качество, пользуясь метриками, подходящими для оценивания задачи мультилейбл классификации.
5.  Сравнить FT (Fine-Tuned) модель с 4-го пункта и выбранную вами Zero-Shot модель (модель, способную делать предсказания на новых данных без дообучения как такового), методы сравнения и метрики на ваше усмотрение.

#### P.S. Для Zero-Shot через Decoder-only лучше всего использовать Instruct модели
#### P.S.S. У вас всё получится, удачи :)
"""

import pandas as pd
from collections import Counter
import plotly.express as px
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/archive.zip

"""# Анализ и подготовка данных"""

df = pd.read_csv('/content/medium_articles.csv')

df.head()

df = df[['title', 'text', 'tags']]

# Отбор 25 самых популярных тегов
all_tags = [tag for tags_list in df["tags"] for tag in eval(tags_list)]
d_tags_counter = Counter(all_tags)
tags, frequencies = list(zip(*d_tags_counter.most_common(n=25)))

fig = px.bar(x=tags, y=frequencies)
fig.update_xaxes(title="tags")
fig.update_yaxes(title="frequencies")
fig.show()

relevant_tags = [
    "Blockchain", "Data Science", "Technology", "Programming", "Poetry",
    "Cryptocurrency", "Machine Learning", "Life", "Bitcoin", "Writing",
    "Politics", "Startup", "Life Lessons", "Self Improvement", "Covid 19",
    "Software Development", "Love", "Python", "Business", "Health",
    "Mental Health", "JavaScript", "Relationships", "Education", "Artificial Intelligence"
]

import ast
def replace_with_relevant_tags(tags_list):
    if isinstance(tags_list, str):
        tags_list = ast.literal_eval(tags_list)

    matched_tags = [tag for tag in tags_list if tag in relevant_tags]
    return matched_tags if matched_tags else np.nan

df['tags'] = df['tags'].apply(replace_with_relevant_tags)

df = df.dropna(subset=df.columns)

df['num_tags'] = df['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)
print(df['num_tags'].value_counts())

# Для мультилейбл классификации лучше отфильтруем строки с одним тегом
df = df[df['num_tags'] > 1]

# Создаем сбалансированную выборку из датасета
balanced_sample = []

for num_tag in [2, 3, 4, 5]:
    # Отбираем все строки с текущим количеством тегов
    subset = df[df['num_tags'] == num_tag]
    sample = subset.sample(n=100, random_state=42)
    balanced_sample.append(sample)

# Объединяем все выборки
final_df = pd.concat(balanced_sample, axis=0)

# Перемешиваем строки
df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)

df

"""# Fine tuning"""

!pip install transformers datasets

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
import torch
from sklearn.preprocessing import MultiLabelBinarizer

tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')
model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=25, problem_type="multi_label_classification")

x = df['text']
yt = df['tags']

from sklearn.model_selection import train_test_split
# First Split for Train and Test
x_train,x_test,y_train,y_test = train_test_split(x, yt, test_size=0.1, random_state=42,shuffle=True)
# Next split Train in to training and validation
x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42,shuffle=True)

def tokenize_function(text):
    return tokenizer(text, padding='max_length', truncation=True, max_length=256)

# Применение токенизации к данным
x_train_tokenized = list(map(tokenize_function, x_train))
x_val_tokenized = list(map(tokenize_function, x_val))
x_test_tokenized = list(map(tokenize_function, x_test))

mlb = MultiLabelBinarizer()
# Применяем one-hot encoding к меткам
y_train_encoded = mlb.fit_transform(y_train).astype('float32')
y_val_encoded = mlb.transform(y_val).astype('float32')
y_test_encoded = mlb.transform(y_test).astype('float32')

# Получаем список всех возможных тегов
all_tags = mlb.classes_

print("Encoded tags for a sample of y_train:")
print(y_train_encoded[:5])  # Показываем первые 5 примеров

print("All tags:", all_tags)  # Показываем все возможные теги

# Создаем словарь для обучающего набора
train_data = {
    "input_ids": [item["input_ids"] for item in x_train_tokenized],
    "attention_mask": [item["attention_mask"] for item in x_train_tokenized],
    "labels": y_train_encoded.tolist()  # Преобразуем массив numpy в список
}

# Аналогично для валидационного набора
val_data = {
    "input_ids": [item["input_ids"] for item in x_val_tokenized],
    "attention_mask": [item["attention_mask"] for item in x_val_tokenized],
    "labels": y_val_encoded.tolist()
}

# Тестовый набор
test_data = {
    "input_ids": [item["input_ids"] for item in x_test_tokenized],
    "attention_mask": [item["attention_mask"] for item in x_test_tokenized],
    "labels": y_test_encoded.tolist()
}

# Преобразуем словари в объекты Dataset
train_dataset = Dataset.from_dict(train_data)
val_dataset = Dataset.from_dict(val_data)
test_dataset = Dataset.from_dict(test_data)

# Создаем TrainingArguments — задаем параметры обучения
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    logging_strategy="steps",
    logging_steps=50,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,
    weight_decay=0.01,
    save_strategy="epoch"
)


# Создаем Trainer с подготовленными данными
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# Запускаем процесс обучения
trainer.train()

trainer.evaluate()

model.save_pretrained('./fine_tuned_bert')

predictions = trainer.predict(test_dataset)

pred_labels = np.argmax(predictions.predictions, axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_test_encoded.argmax(axis=1), pred_labels))

"""Модель не успела нормально обучиться, но я уже не успеваю что-то переделать (нормальный fine tuning с нормальным кол-вом данных после 2 часов обучения истратил все ресурсы gpu и сбросился)

# zero shot
"""

!pip install transformers

from transformers import pipeline

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Тестируем только первый пример
first_text = x.iloc[0]  # Берем первый текст
true_label = yt.iloc[0]  # Истинная метка для первого текста

# Выполнение классификации для первого текстa
result = classifier(first_text, candidate_labels=relevant_tags, multi_label=True)

# Извлечение предсказанной метки
predicted_labels = []
predicted_labels.extend([label for label in result['labels'] if result['scores'][result['labels'].index(label)] >= 0.06])

print("Истинные метки:", true_label)
print("Предсказанные метки:", predicted_labels)
print("Форма истинных меток:", len(true_label), "Форма предсказанных меток:", len(predicted_labels))

"""Далее на cpu слишком долго считается, пришлось оставить так :("""

# Выполнение классификации в пакетном режиме
results = classifier(list(x), candidate_labels=relevant_tags)

# Извлечение предсказанных меток
predicted_labels = []
for result in results:
  predicted_labels.append([label for label in result['labels'] if result['scores'][result['labels'].index(label)] >= 0.06])

predicted_labels = [set(pred) for pred in predicted_labels]  # Преобразуем в наборы для последующего сравнения
predicted_labels

true_labels_set = [set(labels) for labels in yt]
true_labels_set

from sklearn.metrics import f1_score
# Оценка метрик
# Используем макро F1-score для многометочной классификации
f1_macro = f1_score(true_labels_set, predicted_labels, average='macro')

print("F1 Score (Macro):", f1_macro)