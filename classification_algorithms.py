# -*- coding: utf-8 -*-
"""task1_Yankina_Alena.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTChs4q2zv0sPLg8WZd-HLdRTX9jn4sQ

## Домашнее задание 1. Хемометрика
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Сдача домашнего задания 1:**
* Назовите файл по шаблону task1_Ivanov_Ivan.ipynb, подставив свою фамилию и имя. Выполненную работу нужно отправить на почту ivan.s.zlobin@gmail.com, указав в теме письма "Хемометрика и Хемоинформатика, Иванов Иван, 3 курс", подставив свою фамилию, имя и номер курса
 **Работы, присланные иным способом, не принимаются.**
* Прислать нужно **ноутбук в формате `ipynb`**.
* Для выполнения задания **используйте этот ноутбук в качестве основы, ничего не удаляя из него**. Можно добавлять необходимое количество ячеек.
* Комментарии к решению пишите в markdown-ячейках.
* **Код из рассказанных на занятиях ноутбуков** (и из интернета, без злоупотреблений) можно использовать без ограничений.

**Дедлайн по заданию**: 23:59 24 октября 2024 года

**Работы, присланные после дедлайна, не проверяются.**

---

**Баллы за задание:**

* Задача 1 &mdash; 1 балл
* Задача 2 &mdash; 1 балл
* Задача 3 &mdash; 1 балл
* Задача 4 &mdash; 2 балла
* Задача 5 &mdash; 2 балла
* Задача 6 &mdash; 2 балла
* Задача 7 &mdash; 2 балла

Оценка за задание = Сумма баллов (max 10)
Баллы свыше 10 будут добавлены к результату следующего ДЗ.

---

**Проверка задания**

При частичном решении задачи за неё может быть выставлен неполный балл с шагом в 0.5 балла. При проверке кода и комментариев к решению проверяющий будет руководствоваться принципами функциональности, качества кода и качества решения, сформулированных в ПУД курса, а также полнотой и наличием доказательной базы выводов и письменных ответов на поставленные вопросы.

---

В задании вам предлагается решить задачу классификации чая по сортам (классам 1-5) при помощи данных ВЭЖХ-масс-спектрометрии образцов. ВЭЖХ-МС использовался без разделительной колонки, что сильно упрощает обработку данных - в каждый момент времени на выходе колонки ожидается относительно гомогенная смесь, масс-спектр которой мы и будем использовать для дальнейшей работы.

Импортируем файл mzxml (стандартный формат для ВЭЖХ-МС) в Python
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install lxml numpy matplotlib pyteomics
from pyteomics import mzxml
from typing import Tuple, List, Dict
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import re

df = pd.DataFrame()
#с особенностями открытия файла mzxml можно ознакомиться в описании библиотеки
with mzxml.read('/content/drive/MyDrive/tea2_5_MS_no_column_phaseA_ESI_50-1000_Antoshkina_H2O_001.mzXML') as reader:
    #в data хранится масс-спектр для фиксированного значения времени выхода
    for data in reader:
        df = pd.concat([df, pd.DataFrame.from_dict(data)], ignore_index=True)
df

"""В получившейся таблице реально значимыми для нас будут колонки polarity (полярность спектра), retentionTime (время выхода из колонки),	totIonCurrent (суммарный ионный ток), m/z array	(отношение массы к заряду) и intensity array (интенсивность пика)

Вам явно не потребуется весь массив данных - для обработки хватит масс-спектров, получаемых при максимальном ионном токе для данной полярности.

## Задача 1. (1 балл)
Постройте при помощи seaborn масс-спектры обеих полярностей для максимальных значений ионного тока (далее будем называть такие масс-спектры "информативными")
"""

df = df[['polarity','retentionTime', 'totIonCurrent', 'm/z array', 'intensity array']]
df

# Фильтруем по максимальному ионному току для каждой полярности
max_totIonCurrent_pos = df[df['polarity'] == '+']['totIonCurrent'].max()
max_totIonCurrent_neg = df[df['polarity'] == '-']['totIonCurrent'].max()

# Фильтруем строки с максимальным ионным током для положительной полярности
df_pos = df[(df['polarity'] == '+') & (df['totIonCurrent'] == max_totIonCurrent_pos)]

# Фильтруем строки с максимальным ионным током для отрицательной полярности
df_neg = df[(df['polarity'] == '-') & (df['totIonCurrent'] == max_totIonCurrent_neg)]

# Создаем графики масс-спектров
plt.figure(figsize=(6, 6))

# Масс-спектр для положительной полярности
plt.subplot(2, 1, 1)
sns.lineplot(x='m/z array', y='intensity array', data=df_pos)
plt.title('Масс-спектры для положительной полярности')
plt.xlabel('m/z')
plt.ylabel('Интенсивность')

# Масс-спектр для отрицательной полярности
plt.subplot(2, 1, 2)
sns.lineplot(x='m/z array', y='intensity array', data=df_neg)
plt.title('Масс-спектры для отрицательной полярности')
plt.xlabel('m/z')
plt.ylabel('Интенсивность')

plt.tight_layout()
plt.show()

"""## Задача 2. (1 балл)
Соберите общий DataFrame для дальнейшего анализа с данными всех экспериментов в папке "MS". В таблице должны присутствовать как минимум колонки polarity, retentionTime, totIonCurrent, m/z array, intensity array, variety, sample и expN.

_Примечание._ Оператор прибора объяснил, что в первой части названия "tea2_5" указан 5 образец (sample) сорта (variety) 2. Для проверки воспроизводимости данных МС, оператор снимал каждый образец 5 раз, однако с названиями повторных экспериментов не все хорошо - в конце указывается номер эксперимента, иногда принимающий значения в духе "001~4". В рамках кода дальше мы будем воспринимать такой странный номер эксперимента в качестве некоторого идентификатора, записывая его в БД как expN = 14.
"""

def extract_series_from_fname(fname: str):#эта функция корректно распарсит название файла
    stem = fname.rstrip('.mzXML')
    samplename = stem.split('\\')[-1]
    var, sample, expn = samplename.split('_')[0].split('/')[-1], samplename.split('_')[1], samplename.split('_')[-1]
    numlist = expn.split('~')
    first_digit = 10 * int(re.sub(r'\D', '', numlist[0]))
    if len(numlist) > 1:
        second_digit = int(expn[-1])
    else:
        second_digit = 0
    var = str(var)
    sample = int(sample)
    expn = first_digit + second_digit

    return pd.Series([var, sample, expn], index=['variety','sample', 'expN'])


def parse_data(path: Path):
        df = pd.DataFrame(columns =['polarity', 'retentionTime', 'totIonCurrent', 'm/z array', 'intensity array', 'variety', 'sample', 'expN'])
        # Открываем и читаем файл mzXML
        with mzxml.read(path) as reader:
          for raw_data in reader:
            polarity = raw_data['polarity']
            retentionTime = raw_data['retentionTime']
            totIonCurrent = raw_data['totIonCurrent']
            mz_array = raw_data['m/z array']
            intensity_array = raw_data['intensity array']

            # Добавляем данные в DataFrame
            df = pd.concat([df, pd.DataFrame({
                'polarity': [polarity],
                'retentionTime': [retentionTime],
                'totIonCurrent': [totIonCurrent],
                'm/z array': [mz_array],
                'intensity array': [intensity_array]
            })], ignore_index=True)
        return df


def create_sample_dataframe(path: Path):
    metadata_series = extract_series_from_fname(path)
    ms_data = parse_data(path)
    for key in metadata_series.keys():
        ms_data[key] = metadata_series[key]
    return ms_data

"""Код нижу должен будет корректно работать после вашего дополнения функции parse_data, если вы работаете при помощи Jupyter Notebook на Windows. Если нет - проблема легко решается по коду ошибки при помощи интернета (:"""

all_df = pd.DataFrame()
train_datadir = Path("/content/drive/MyDrive/MS").iterdir()
for sample_file in train_datadir:
    all_df = pd.concat([all_df, create_sample_dataframe(str(sample_file))], ignore_index=True)
all_df

"""## Задача 3. (1 балл)
Изобразите все полученные информативные масс-спектры (раздельно для разных полярностей) с наложением, используя разные цвета для различных сортов чая.
"""

# Функция для визуализации информативных масс-спектров
def plot_mass_spectra_informative(df):
    varieties = ['tea1', 'tea2', 'tea3', 'tea4', 'tea5']

    # Масс-спектры для положительной полярности (+)
    plt.figure(figsize=(10, 6))
    plt.title("Информативные масс-спектры для положительной полярности (+)")
    plt.xlabel("m/z")
    plt.ylabel("Интенсивность")

    for variety in varieties:
        # Фильтруем данные для текущего сорта чая
        filtered_df = df[df['variety'] == variety]

        # Ищем максимальный ионный ток для положительной полярности
        max_totIonCurrent_pos = filtered_df[filtered_df['polarity'] == '+']['totIonCurrent'].max()

        # Фильтруем строки с максимальным ионным током для положительной полярности
        df_pos = filtered_df[(filtered_df['polarity'] == '+') & (filtered_df['totIonCurrent'] == max_totIonCurrent_pos)]

        # Собираем все точки m/z и интенсивности для данного сорта чая
        mz_all = []
        intensity_all = []

        for i, row in df_pos.iterrows():
            mz_array = np.array(row['m/z array'])
            intensity_array = np.array(row['intensity array'])

            mz_all.extend(mz_array)
            intensity_all.extend(intensity_array)

        # Строим график
        sns.lineplot(x=mz_all, y=intensity_all, label=variety)

    plt.legend(title="Сорт чая")
    plt.grid(True)
    plt.show()

    # Масс-спектры для отрицательной полярности (-)
    plt.figure(figsize=(10, 6))
    plt.title("Информативные масс-спектры для отрицательной полярности (-)")
    plt.xlabel("m/z")
    plt.ylabel("Интенсивность")

    for variety in varieties:
        filtered_df = df[df['variety'] == variety]

        # Ищем максимальный ионный ток для отрицательной полярности
        max_totIonCurrent_neg = filtered_df[filtered_df['polarity'] == '-']['totIonCurrent'].max()

        # Фильтруем строки с максимальным ионным током для отрицательной полярности
        df_neg = filtered_df[(filtered_df['polarity'] == '-') & (filtered_df['totIonCurrent'] == max_totIonCurrent_neg)]

        # Собираем все точки m/z и интенсивности для данного сорта чая
        mz_all = []
        intensity_all = []

        for i, row in df_neg.iterrows():
            mz_array = np.array(row['m/z array'])
            intensity_array = np.array(row['intensity array'])

            mz_all.extend(mz_array)
            intensity_all.extend(intensity_array)

        # Строим график
        sns.lineplot(x=mz_all, y=intensity_all, label=variety)

    plt.legend(title="Сорт чая")
    plt.grid(True)
    plt.show()

# Вызов функции для визуализации
plot_mass_spectra_informative(all_df)

"""Изучите спектры. Достаточно ли сильно отличаются спектры чая разных сортов? Можно ли сделать какие-то выводы "на глаз"?

**ОТВЕТ:** Глядя на полученные спектры, можно заметить, что сорта чая действительно отличаются друг от друга, но не все различия очевидны на первый взгляд. У каждого сорта наблюдаются какие-то уникальные пики, не свойственные остальным. Однако в основном их спектры много пересекаются, и отличить их "на глаз" достаточно сложно. Пики накладываются друг на друга, и поэтому сказать с уверенностью, что они значительно различаются, только по визуальному сравнению, сложно. В общем, можно сделать предварительный вывод, что кое-какие различия есть, но чтобы точно определить каждый сорт, нужна работа с алгоритмами для классификации.

## Задача 4. (2 балла)
Разделите обучающий датасет на обучающую и тестовую выборку в пропорции 60/40. В качестве фичей используйте информативный спектр для образца, в качестве таргета - сорт (variety). Не забудьте стратифицировать выборки по классам!
Обучите для "пристрелки" ваш любимый классифицирующий алгоритм, используя в качестве целевой метрики accuracy, и измерьте метрики классификации на тесте.

_Примечание._ Чтобы обеспечить "одномерный" вход модели, на данном этапе достаточно сконкатенировать векторы m/z и intensity.
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Функция для подготовки данных
def prepare_data(df):
    # Находим минимальную длину массива m/z среди всех строк
    min_length = min(df['m/z array'].apply(len).min(), df['intensity array'].apply(len).min())

    X = []
    y = []

    for i, row in df.iterrows():
        # Преобразуем массивы в numpy
        mz_array = np.array(row['m/z array'])
        intensity_array = np.array(row['intensity array'])

        # Обрезаем массивы до минимальной длины
        mz_array_trimmed = mz_array[:min_length]
        intensity_array_trimmed = intensity_array[:min_length]

        # Объединяем обрезанные массивы в единый вектор
        combined_feature = np.concatenate([mz_array_trimmed, intensity_array_trimmed])

        # Добавляем фичи и сорт чая в итоговые списки
        X.append(combined_feature)
        y.append(row['variety'])

    return np.array(X), np.array(y)

# 1. Подготовка данных
X, y = prepare_data(all_df)

# 2. Разбиение на обучающую и тестовую выборки с учетом стратификации
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# 3. Обучение
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# 4. Предсказание и оценка метрик
y_pred = clf.predict(X_test)

# Оценка accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Полный отчет по классификации
print("Classification report:")
print(classification_report(y_test, y_pred))

"""О чем говорит полученное значение метрики? Почему вы выбрали данный алгоритм классификации?

**ОТВЕТ:** Значение метрики показало, что модель достаточно успешно распознает сорта чая по спектрам. Я выбрала Random Forest, потому что это наиболее гибкий и простой в использовании алгоритм, быстро настраивается и хорошо справляется с разными признаками, как у нас. Плюс, он не так сильно переобучается и дает хорошие результаты с самого начала.

## Задача 5. (2 балла)
Решите задачу классификации в условиях задачи 4, используя PCA в качестве алгоритма понижения размерности входного вектора. Самостоятельно подберите оптимальный размер пространства PCA-признаков и используйе **тот же** классифицирующий алгоритм для решения классификации.
"""

from sklearn import decomposition
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Функция для подготовки данных
def prepare_data(df):
    # Находим минимальную длину массива m/z среди всех строк
    min_length = min(df['m/z array'].apply(len).min(), df['intensity array'].apply(len).min())

    X = []
    y = []

    for i, row in df.iterrows():
        # Преобразуем массивы в numpy
        mz_array = np.array(row['m/z array'])
        intensity_array = np.array(row['intensity array'])

        # Обрезаем массивы до минимальной длины
        mz_array_trimmed = mz_array[:min_length]
        intensity_array_trimmed = intensity_array[:min_length]

        # Объединяем обрезанные массивы в единый вектор
        combined_feature = np.concatenate([mz_array_trimmed, intensity_array_trimmed])

        # Добавляем фичи и сорт чая в итоговые списки
        X.append(combined_feature)
        y.append(row['variety'])

    return np.array(X), np.array(y)

# Разделим данные на фичи и целевую переменную
X, y = prepare_data(all_df)

# 1. Разделим данные на тренировочную и тестовую выборки с учетом стратификации
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# 2. Применение PCA для понижения размерности
# Сначала определим оптимальное количество компонент, чтобы объяснить достаточную долю дисперсии
pca = PCA()
pca.fit(X_train)

# Визуализация накопленной объяснённой дисперсии
cumulative_variance = pca.explained_variance_ratio_.cumsum()
plt.figure(figsize=(8,6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')
plt.axhline(y=0.90, color='r', linestyle='-')  # Линия для 90% объяснённой дисперсии
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

# 3. Найдём оптимальное число компонент
optimal_components = next(i for i, var in enumerate(cumulative_variance, 1) if var >= 0.90)
print(f"Оптимальное число компонент: {optimal_components}")

# 4. Применяем PCA с оптимальным числом компонент
pca_optimal = PCA(n_components=optimal_components)
X_train_pca = pca_optimal.fit_transform(X_train)
X_test_pca = pca_optimal.transform(X_test)

# 5. Обучаем классификатор
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_pca, y_train)

# 6. Оценка модели на тестовых данных
y_pred = clf.predict(X_test_pca)

# 7. Оценка качества классификации
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification report:")
print(classification_report(y_test, y_pred))

"""Как изменился результат? Почему?

**ОТВЕТ:** Применение РСА улучшило точность классификации с 0.8524 до 0.8852, так как оно выделило наиболее важные признаки, уменьшив шум и ненужные данные. Это позволило модели более точно различать сорта чая, улучшив показатели точности и полноты.

## Задача 6. (2 балла)
Предложите собственный алгоритм снижения размерности входного вектора для модели и постройте классификатор в условиях задачи 4, используя **тот же** классифицирующий алгоритм.

_Подсказка._ Смотрели ли вы на весь масс-спектр во время сравнения образцов? Все ли наблюдаемые пики можно считать информативными?
"""

# Функция для фильтрации по интенсивности и частоте появления
def filter_spectra(df, intensity_threshold=0.05, frequency_threshold=0.5):
    filtered_df = pd.DataFrame()

    for index, row in df.iterrows():
        mz_array = np.array(row['m/z array'])
        intensity_array = np.array(row['intensity array'])

        # Фильтруем по интенсивности
        max_intensity = np.max(intensity_array)
        relevant_indices = intensity_array >= max_intensity * intensity_threshold

        # Оставляем только пики, прошедшие фильтрацию
        filtered_mz = mz_array[relevant_indices]
        filtered_intensity = intensity_array[relevant_indices]

        # Если после фильтрации есть пики, добавляем их в новый DataFrame
        if len(filtered_mz) > 0:
            filtered_df = pd.concat([filtered_df, pd.DataFrame({
                'm/z array': [filtered_mz],
                'intensity array': [filtered_intensity],
                'variety': row['variety']
            })], ignore_index=True)

    return filtered_df

# Применим фильтрацию
filtered_df = filter_spectra(all_df)

# Разделение выборки
X, y = prepare_data(filtered_df)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Обучение классификатора
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Оценка модели
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification report:")
print(classification_rep)

"""Как изменился результат? Почему? Какую из трех построенных моделей, по вашему мнению, есть смысл использовать далее?

**ОТВЕТ:**
Результат улучшился до ~0.93 благодаря фильтрации неинформативных пиков, что уменьшило шум и повысило точность. Наиболее целесообразно будет использовать далее эту последнюю модель, так как она показала лучшую производительность.

## Задача 7. (2 балла)
Выберите наболее подходящий, на ваш взгляд, способ понижения размерности из реализованных выше. Используя его, обучите и сравните на тесте разные классифицирующие алгоритмы из Scikit-learn (не менее трех). Для каждого из импользуемых алгоритмов реализуйте подбор оптимальных гиперпараметров (как минимум, перебор пяти значений одного непрерывного гиперпараметра модели).
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split, GridSearchCV

# Разделение выборки
X, y = prepare_data(filtered_df)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# 1. ExtraTreesClassifier
extra_trees_params = {'n_estimators': [50, 100, 200, 300, 500]}  # Количество деревьев
extra_trees = ExtraTreesClassifier()
extra_trees_grid = GridSearchCV(extra_trees, extra_trees_params, cv=5, scoring='accuracy', n_jobs=-1)
extra_trees_grid.fit(X_train, y_train)
extra_trees_best = extra_trees_grid.best_estimator_
y_pred_extra_trees = extra_trees_best.predict(X_test)
extra_trees_accuracy = accuracy_score(y_test, y_pred_extra_trees)
extra_trees_report = classification_report(y_test, y_pred_extra_trees)

# Вывод результатов
print(f"Extra Trees Accuracy: {extra_trees_accuracy}")
print(f"Extra Trees Classification Report: \n{extra_trees_report}")

# 2. KNN
knn_params = {'n_neighbors': [3, 5, 7, 9, 11]}
knn = KNeighborsClassifier()
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='accuracy', n_jobs=-1)
knn_grid.fit(X_train, y_train)
knn_best = knn_grid.best_estimator_
y_pred_knn = knn_best.predict(X_test)
knn_accuracy = accuracy_score(y_test, y_pred_knn)
knn_report = classification_report(y_test, y_pred_knn)

# Вывод результатов
print(f"KNN Accuracy: {knn_accuracy}")
print(f"KNN Classification Report: \n{knn_report}")

# 3. Random Forest
rf_params = {'n_estimators': [50, 100, 150, 200, 250]}
rf = RandomForestClassifier(random_state=42)
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_
y_pred_rf = rf_best.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_report = classification_report(y_test, y_pred_rf)

# Вывод результатов
print(f"Random Forest Accuracy: {rf_accuracy}")
print(f"Random Forest Classification Report: \n{rf_report}")

from sklearn.naive_bayes import MultinomialNB

# 4. MultinomialNB
nb_params = {'alpha': [0.01, 0.1, 0.5, 1, 5]}  # Гиперпараметр сглаживания
nb = MultinomialNB()
nb_grid = GridSearchCV(nb, nb_params, cv=5, scoring='accuracy', n_jobs=-1)
nb_grid.fit(X_train, y_train)
nb_best = nb_grid.best_estimator_
y_pred_nb = nb_best.predict(X_test)
nb_accuracy = accuracy_score(y_test, y_pred_nb)
nb_report = classification_report(y_test, y_pred_nb)

# Вывод результатов
print(f"Naive Bayes Accuracy: {nb_accuracy}")
print(f"Naive Bayes Classification Report: \n{nb_report}")

"""Какая из моделей оказалась лучше? Почему вы выбрали именно эти модели? Чем мотивирован выбор перебираемых гиперпараметров? Каким образом вы определили шаг и пределы для перебора этих гиперпараметров?

**ОТВЕТ:** Random Forest оказалась лучшей моделью с точностью 0.9309, так как она эффективно работает с нелинейно разделимыми данными и снижает риск переобучения. Модели были выбраны на основе их устойчивости к шуму (Random Forest и Extra Trees), простоты (KNN), а также для разнообразия был добавлен Naive Bayes, чтобы оценить его эффективность на данных. Однако Naive Bayes показал совсем низкую точность (0.3397). Random Forest и Extra Trees похожи по принципу работы, но несмотря на это я выбрала сразу обе модели, чтобы сравнить чем же все таки будут отличаться их результаты. Гиперпараметры для ансамблевых моделей — количество деревьев, для KNN — количество соседей, а для Naive Bayes был подобран гиперпараметр сглаживания alpha. Эти параметры были выбраны, поскольку они напрямую влияют на производительность моделей. Диапазоны перебора параметров (50-500 для деревьев, 3-15 для соседей и 0.01-5 для alpha) были выбраны для охвата как простых, так и более сложных моделей, с шагом, который позволяет отслеживать значительные изменения в результатах.
"""