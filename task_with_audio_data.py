# -*- coding: utf-8 -*-
"""КР_Янкина_Алёна_Алексеевна.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PyS7Lgy11zhh4iiChqjoiZP0hI_OOexk

# Контрольная работа №1
Работа сдается в гугл-форму, которая будет закреплена в telegram конференции группы

Работа при сдаче в гугл форму должна называться КР_Фамилия_Имя_Отчество.ipynb

Перед сдачей просьба не стирать вывод ячеек для того, чтобы преподаватель заново не запускал ваш код

Задание.

1. Загрузить набор данных, приложенный в задании.
2. Произведите обработку данных и вычисление признаков речевых сигналов (Построение мел-спектрограмм и т.д.)
3. Выполните аугментацию данных (приложите визуализации).
4. (Дополнительный бал) Обучите модель распознавания эмоционального настроения по звуку. Можно использовать как Transfer Learning, так и спроектировать собственное решение. Прогоните распознование на случайных данных из выборки (выберите методов random)

### P.S.
* 4-й пункт на дополнительные баллы, позволит перекрыть неправильные ответы в тесте.
* Прокомментируйте обработку данных и аугментацию данных

### P.S.S У вас всё получится, удачи :)

# Загрузка и обработка данных
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import librosa
import random
import librosa.display
from IPython.display import Audio

!cp '/content/drive/MyDrive/Alena Yankina/Crema.zip' Crema.zip
!unzip Crema.zip -d data/

# Получаем список всех файлов
crema_directory_list = os.listdir('data/Crema/')

# Инициализируем два пустых списка для хранения эмоций и путей к файлам
file_emotion = []
file_path = []

print("Количество аудиофайлов:")
print(len(crema_directory_list))

# Перебираем каждый файл в списке
for file in crema_directory_list:
    # Конструируем полный путь к файлу и добавляем его в список file_path
    file_path.append('data/Crema/' + file)

    # Разделяем имя файла для извлечения меток эмоций
    part = file.split('_')

    if part[2] == 'SAD':
        file_emotion.append('sad')
    elif part[2] == 'ANG':
        file_emotion.append('angry')
    elif part[2] == 'DIS':
        file_emotion.append('disgust')
    elif part[2] == 'FEA':
        file_emotion.append('fear')
    elif part[2] == 'HAP':
        file_emotion.append('happy')
    elif part[2] == 'NEU':
        file_emotion.append('neutral')
    else:
        file_emotion.append('Unknown')

# Создаем DataFrame для меток эмоций файлов
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# Создаем DataFrame для путей к файлам
path_df = pd.DataFrame(file_path, columns=['Path'])

# Объединяем оба DataFrame по столбцам
Crema_df = pd.concat([emotion_df, path_df], axis=1)

# Выводим первые несколько строк объединенного DataFrame
Crema_df.head()

"""# Аугментация"""

# Функция для добавления шума к аудиоданным
def noise(data):
    # Генерируем амплитуду шума, пропорциональную максимальному значению данных
    noise_amp = 0.035 * np.random.uniform() * np.amax(data)
    # Добавляем шум к данным, используя нормальное распределение
    data = data + noise_amp * np.random.normal(size=data.shape[0])
    return data

# Функция для растяжения временной оси аудиоданных
def stretch(data, rate=0.8):
    # Применяем растяжение времени с указанным коэффициентом
    return librosa.effects.time_stretch(data, rate=rate)

# Функция для сдвига данных аудиосигнала
def shift(data):
    # Генерируем случайный диапазон сдвига от -5 до 5 секунд и умножаем на 1000 для получения сдвига в миллисекундах
    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)
    # Сдвигаем данные
    x = np.roll(data, shift_range)
    return x

# Функция для изменения высоты тона аудиосигнала
def pitch(data, sampling_rate, pitch_factor=0.7):
    # Изменяем высоту тона с указанным коэффициентом
    x = librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)
    return x

# Загружаем путь к аудиофайлу из DataFrame
path = np.array(Crema_df.Path)[1]
print(path)

# Загружаем аудиофайл
data, sampling_rate = librosa.load(path)

# Выводим загруженные данные
print(data)  # Данные аудиосигнала
print(type(data))  # Тип данных
print(data.shape)  # Форма данных (размерность массива)
print(sampling_rate)  # Частота дискретизации аудиофайла

# Оригинал
plt.figure(figsize=(14,4))
librosa.display.waveshow(y=data, sr=sampling_rate)
Audio(path)

# Добавление шума
x = noise(data)
plt.figure(figsize=(14,4))
librosa.display.waveshow(y=x, sr=sampling_rate)
Audio(x, rate=sampling_rate)

# Растяжение
x = stretch(data)
print("Stretched Data: ")
print(x)
plt.figure(figsize=(14,4))
librosa.display.waveshow(y=x, sr=sampling_rate)
Audio(x, rate=sampling_rate)

# Сдвиг
x = shift(data)
plt.figure(figsize=(14,4))
librosa.display.waveshow(y=x, sr=sampling_rate)
Audio(x, rate=sampling_rate)

# Изменение высоты тона
x = pitch(data, sampling_rate)
plt.figure(figsize=(14,4))
librosa.display.waveshow(y=x, sr=sampling_rate)
Audio(x, rate=sampling_rate)

"""# Вычисление признаков речевых сигналов"""

def extract_features(data, type_data):
    # ZCR
    result = np.array([])
    x = librosa.feature.zero_crossing_rate(y=data)
    zcr = np.mean(x.T, axis=0)
    result = np.hstack((result, zcr))

    # Root Mean Square Value
    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)
    result = np.hstack((result, rms))

    # MFCC
    x = librosa.feature.mfcc(y=data, sr=sampling_rate)
    mfcc = np.mean(x.T, axis=0)
    result = np.hstack((result, mfcc))

    # MelSpectogram
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sampling_rate).T, axis=0)
    result = np.hstack((result, mel))

    return result

def get_features(path):
    result = np.array([])
    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)

    # данные без аугментации
    res1 = extract_features(data, type_data = "Normal")
    result = np.array(res1)

    # данные с добавленным шумом
    noise_data = noise(data)
    res2 = extract_features(noise_data, type_data = "Noisy")
    result = np.vstack((result, res2))

    # данные с изменением тона и растяжением звука
    new_data = stretch(data)
    data_stretch_pitch = pitch(new_data, sample_rate)
    res3 = extract_features(data_stretch_pitch, type_data = "Stretched and Pitched")
    result = np.vstack((result, res3))

    return result

X, Y = [], []
for path, emotion in zip(Crema_df.Path, Crema_df.Emotions):
    feature = get_features(path)
    for ele in feature:
        X.append(ele)
        Y.append(emotion)

print("Input Variable: ")
print(len(X))
print("Output Variable: ")
print(len(Y))
print(Crema_df.Path.shape)

Features = pd.DataFrame(X)
Features['labels'] = Y
Features.to_csv('features.csv', index=False)
Features.head()

"""# Обучение модели"""

!pip install keras

!pip install np_utils

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization
from keras.utils import to_categorical
import np_utils
from keras.callbacks import ModelCheckpoint

# Подготовка данных
X = Features.iloc[: ,:-1].values
Y = Features['labels'].values

encoder = OneHotEncoder()
Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()

# Разделение данных
x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

# Стандартизация данных
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

# Обеспечение совместимости данных с моделью
x_train = np.expand_dims(x_train, axis=2)
x_test = np.expand_dims(x_test, axis=2)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

model=Sequential()
model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Dropout(0.2))

model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Flatten())
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(units=6, activation='softmax'))
model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

model.summary()

rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)
history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])

print("Accuracy of our model on test data : " , model.evaluate(x_test,y_test)[1]*100 , "%")

# Предсказание на тестовых данных
pred_test = model.predict(x_test)
y_pred = encoder.inverse_transform(pred_test)

y_test = encoder.inverse_transform(y_test)

df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])
df['Predicted Labels'] = y_pred.flatten()
df['Actual Labels'] = y_test.flatten()

df.head(10)

print(classification_report(y_test, y_pred))

"""Точность получилась не самая лучшая, но хоть что-то, модель определяет правильно каждое второе эмоциональное настроение)"""