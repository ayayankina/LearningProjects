# -*- coding: utf-8 -*-
"""Conditional GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aD4dj-WNkIqd4-uYsgqIZty4OnU3cmf

### Задание 1. Реализуйте свой GAN или VAE(архитектура может быть похожа на архитектуру написанную на семинаре) на датасете, не входящем в перечень torchvision.
"""

! pip install kaggle

! kaggle datasets download rayeed045/american-sign-language-digit-dataset

! unzip american-sign-language-digit-dataset.zip

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
import shutil

# Подготовка данных(чтобы в папках для каждой цифры были сразу изображения без лишних файлов внутри)

source_path = "/content/American Sign Language Digits Dataset"
destination_path = "/content/processed_dataset"

# Проверка, что каталог для преобразованного датасета существует
os.makedirs(destination_path, exist_ok=True)

# Проход по основным папкам классов
for class_folder in os.listdir(source_path):
    class_path = os.path.join(source_path, class_folder)
    if os.path.isdir(class_path):
        # Создание папки для текущего класса в целевом каталоге
        os.makedirs(os.path.join(destination_path, class_folder), exist_ok=True)

        # Копирование всех изображений из вложенных папок в папку текущего класса
        for root, _, files in os.walk(class_path):
            for file in files:
                if file.endswith(('.jpeg')):
                    shutil.copy(
                        os.path.join(root, file),
                        os.path.join(destination_path, class_folder, file)
                    )

print("Структура датасета преобразована!")

# Используем gpu
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Гиперпараметры
noise_channels = 256
num_classes = 10  # Для цифр 0-9
image_size = 64
image_channels = 1  # Grayscale
batch_size = 32
learning_rate = 0.0002
epochs = 100
gen_features = 64
disc_features = 64

torch.manual_seed(123)

transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

dataset = datasets.ImageFolder(root="/content/processed_dataset", transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Посмотрим все классы нашего набора данных
plt.figure(figsize=(10, 2))

for i in range(num_classes):
    # Найти изображения одного класса
    class_images = []
    for images, labels in dataloader:
        class_images.extend(images[labels == i])
        if len(class_images) >= 1:
            break

    # Выбираем первое изображение из каждого класса
    image = class_images[0].numpy().transpose(1, 2, 0)  # преобразуем тензор обратно в формат изображения
    image = (image * 0.5 + 0.5)  # Денормализация, т.к. изображения были нормализованы в [0, 1]

    plt.subplot(1, num_classes, i + 1)
    plt.imshow(image, cmap="gray_r")
    plt.title(f"Class {i}")
    plt.axis("off")

plt.show()

class Generator(nn.Module):
    def __init__(self, noise_channels, image_channels, features):
        super(Generator, self).__init__()
        self.label_embedding = nn.Embedding(num_classes, num_classes)
        self.model = nn.Sequential(
            nn.ConvTranspose2d(noise_channels + num_classes, features * 16, kernel_size=4, stride=1, padding=0),
            nn.ReLU(),

            nn.ConvTranspose2d(features * 16, features * 8, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 8),
            nn.ReLU(),

            nn.ConvTranspose2d(features * 8, features * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 4),
            nn.ReLU(),

            nn.ConvTranspose2d(features * 4, features * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.ReLU(),

            nn.ConvTranspose2d(features * 2, image_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )

    def forward(self, noise, labels):
        label_emb = self.label_embedding(labels).unsqueeze(2).unsqueeze(3)  # Embedding -> shape (batch_size, num_classes, 1, 1)
        label_emb = label_emb.expand(-1, -1, noise.size(2), noise.size(3))  # Align label dimensions with noise
        input = torch.cat((noise, label_emb), 1)  # Concatenate along channel dimension
        return self.model(input)


class Discriminator(nn.Module):
    def __init__(self, image_channels, features):
        super(Discriminator, self).__init__()
        self.label_embedding = nn.Embedding(num_classes, num_classes)
        self.model = nn.Sequential(
            nn.Conv2d(image_channels + num_classes, features, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),

            nn.Conv2d(features, features * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.LeakyReLU(0.2),

            nn.Conv2d(features * 2, features * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 4),
            nn.LeakyReLU(0.2),

            nn.Conv2d(features * 4, features * 8, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 8),
            nn.LeakyReLU(0.2),

            nn.Conv2d(features * 8, 1, kernel_size=4, stride=2, padding=0),
            nn.Sigmoid(),
        )

    def forward(self, img, labels):
        label_emb = self.label_embedding(labels).unsqueeze(2).unsqueeze(3)  # Shape: (batch_size, num_classes, 1, 1)
        label_emb = label_emb.expand(-1, -1, img.size(2), img.size(3))  # Expand to match image dimensions
        img = torch.cat((img, label_emb), 1)  # Concatenate along channel dimension
        return self.model(img)

# Инициализация моделей
generator = Generator(noise_channels, image_channels, gen_features).to(device).to(device)
discriminator = Discriminator(image_channels, disc_features).to(device)

loss_function = nn.BCELoss()
optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# Обучение
for epoch in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader):

        batch_size = imgs.size(0)
        imgs, labels = imgs.to(device), labels.to(device)

        # Метки для реальных и сгенерированных изображений
        valid = (torch.ones((batch_size, 1)) * 0.9).to(device)
        fake = (torch.ones((batch_size, 1)) * 0.1).to(device)

        # Обучение генератора
        optimizer_g.zero_grad()
        noise = torch.randn(batch_size, noise_channels, 1, 1).to(device)
        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)
        gen_imgs = generator(noise, gen_labels)
        g_loss = loss_function(discriminator(gen_imgs, gen_labels).view(-1, 1), valid)
        g_loss.backward()
        optimizer_g.step()

        # Обучение дискриминатора
        optimizer_d.zero_grad()
        real_loss = loss_function(discriminator(imgs, labels).view(-1, 1), valid)
        fake_loss = loss_function(discriminator(gen_imgs.detach(), gen_labels).view(-1, 1), fake)
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_d.step()

        if i % 50 == 0:
            print(f"Epoch [{epoch}/{epochs}] Batch [{i}/{len(dataloader)}] "
                  f"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

    # Сохранение модели каждые 5 эпох
    if (epoch + 1) % 5 == 0:
      torch.save(generator.state_dict(), f"generator_epoch_{epoch}.pth")
      torch.save(discriminator.state_dict(), f"discriminator_epoch_{epoch}.pth")

# Вывод сгенерированных изображений каждого класса
def generate_images_per_class(generator, num_classes=10):
    generator.eval()

    # Генерация 1 шума для каждого класса
    noise = torch.randn(num_classes, noise_channels, 1, 1).to(device)
    class_labels = torch.arange(0, num_classes).to(device)  # Метки от 0 до num_classes - 1

    with torch.no_grad():
        gen_imgs = generator(noise, class_labels).cpu()

    gen_imgs = (gen_imgs + 1) / 2  # Денормализация изображений

    # Отображение каждого изображения
    plt.figure(figsize=(10, 2))  # Установка размера графика
    for i in range(num_classes):
        plt.subplot(1, num_classes, i + 1)
        plt.imshow(gen_imgs[i].squeeze(0), cmap="gray_r")
        plt.title(f"Class {i}")
        plt.axis("off")
    plt.show()

generate_images_per_class(generator, num_classes=10)

"""### Напишите вывод о ваших экспериментах с набором данных, архитектурой и ее параметрами.

Я преобразовала изображения из датасета, поменяв размерность на 64 и сделав одноцветный канал, что помогло оптимизировать модель, убрав лишние нагрузки на обработку оригинальных данных. Архитектуру я использовала почти ту же, что и в прошлой самостоятельной работе: генератор и дискриминатор написаны на архитектурах сверточной нейронной сети. Однако вместо стандартного GAN я реализовала Conditional GAN (CGAN), благодаря чему я смогла управлять тем, какой класс (жест) генерировать и вывести изображения каждого класса целенаправленно.

О параметрах:
Сначала я выбрала noise_channels = 100, но качество генерации получалось слишком плохим, так что чтобы попробовать его улучшить и добиться более точного отображения деталей, я установила noise_channels = 256.
batch_size и learning_rate были выбраны такими же как в семинаре и сразу показали хорошую эффективность, так что с ними я не экспериментировала.
Количество эпох: я пробовала обучить модель за 20, 50 и 100 эпох. В первых двух случаях модель уже генерировала очертания, похожие на жесты рук, но на картинках также все еще присутствовал лишний шум. После 100 эпох качество генерируемых картинок мне показалось достаточно хорошим и понятным.
"""